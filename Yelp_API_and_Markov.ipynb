{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all the imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg as LA\n",
    "import random\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as TVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "YELP DATA CLASS\n",
    "- used to load yelp json files for reviews and business\n",
    "- class creates a mini-database for users of the class\n",
    "- on init, class extracts all reviews for a business or category\n",
    "'''\n",
    "\n",
    "class YelpData:\n",
    "    '''\n",
    "    HOW TO USE\n",
    "    - yelp variable below initializes instance of the YelpData class\n",
    "    - first init takes some time b/c it loads 4+ million reviews and 15k businesses\n",
    "    - after it loads, you NEVER need to load the data again\n",
    "    - you can interact with the following 3 variables\n",
    "        - yelp.categories - gives you all the categories for businesses\n",
    "        - yelp.businesses - gives you all the businesses and their details\n",
    "        - yelp.reviews - gives you all the reviews for businesses\n",
    "    '''\n",
    "    def __init__(self,path):\n",
    "        '''initiatize yelp class and load the appropriate data once'''\n",
    "        self.path = path\n",
    "        self.business = 'business.json'\n",
    "        self.review = 'review.json'\n",
    "        # Use these variables to access data once instance of class initialized\n",
    "        self.businesses = self._business_details()\n",
    "        self.categories = self._business_categories()\n",
    "        self.reviews = self._review_details()\n",
    "    \n",
    "    def see_files(self):\n",
    "        '''simple method to see the files in the yelp directory'''\n",
    "        os.chdir(path)\n",
    "        return os.listdir()\n",
    "    \n",
    "    def retrieve_all_data(self,f):\n",
    "        '''retrieves all the data from a file given the path of the yelp directory'''\n",
    "        os.chdir(self.path)\n",
    "        with open(f) as json_data:\n",
    "            data = [json.loads(line) for line in json_data]\n",
    "        return data\n",
    "    '''\n",
    "    NOTE THE UNDERSCORE IN FOLLOWING METHODS\n",
    "    - in another language, these methods would be declared private, but not in python\n",
    "    - As Guido notes, \"we are all consenting adults here\".\n",
    "    - Underscore signifies private suggestion, but not mandatory\n",
    "    \n",
    "    - Why should these methods be private? \n",
    "        - they load data from files, and the data in those files does not change \n",
    "        - therefore, you need to load the data only once\n",
    "        - Each subsequent load is 1) inefficient and 2) consumes memory\n",
    "        - the data should be loaded once, when the class is initialized\n",
    "        - users should interact with the loaded data\n",
    "        - functions similarlry to an application cache\n",
    "    '''\n",
    "    def _b_dict(self,d,cols):\n",
    "        return dict((k,v) for k,v in d.items() if k in cols)\n",
    "    \n",
    "    def _business_details(self):\n",
    "        '''flatten structure to get all details of business, with key as index'''\n",
    "        businesses = self.retrieve_all_data(self.business)\n",
    "        cols = ('address','business_id','city','name','review_count','stars')\n",
    "        business_df = pd.DataFrame([self._b_dict(d,cols) for d in businesses])\n",
    "        return business_df.set_index('business_id')\n",
    "\n",
    "    def _business_categories(self):\n",
    "        '''categories will function as a table in 1NF w/ business_id,category'''\n",
    "        businesses = self.retrieve_all_data(self.business)\n",
    "        cols = ('business_id','categories')\n",
    "        business_cats = [self._b_dict(d,cols) for d in businesses]\n",
    "        # b/c business id is one to many, we need to cycle business ids to each category\n",
    "        # then we need to flatten the list so we can make it a dataframe\n",
    "        # this is cryptic, but again, this method is not for an end user of the class\n",
    "        flattened_cats = list(it.chain(*[list(zip(it.cycle([d['business_id']]),\n",
    "                                                  d['categories'])) for d in business_cats]))\n",
    "        return pd.DataFrame(flattened_cats,columns=['business_id', 'category'])\n",
    "    \n",
    "    def _review_details(self):\n",
    "        '''flatten structure to get all details of users'''\n",
    "        reviews = self.retrieve_all_data(self.review)\n",
    "        cols = ('business_id','review_id','stars','text','cool','funny','useful')\n",
    "        review_df = pd.DataFrame([self._b_dict(d,cols) for d in reviews])\n",
    "        return review_df.set_index('review_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "MARKOV MODEL CLASS\n",
    "- Used to generate random sentences from input data\n",
    "- Input data we use for this is yelp dataset\n",
    "'''\n",
    "class MarkovModel:\n",
    "    '''\n",
    "    HOW TO USE\n",
    "    - MarkovModel built from scratch\n",
    "    - Does not use scapy or nltk (mainly to demonstrate properties) \n",
    "    - initialize by passing a list of sentences (used for corpus) and the order for n-gram creation\n",
    "    - the word builders are two basic methods to build / mimic a transition matrix\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,sentence_vec,order):\n",
    "        self.sentence_vec = sentence_vec\n",
    "        self.corpus = ' '.join(sentence_vec).replace('\\n',' ')\n",
    "        self.order = order\n",
    "        self.punc_split = r\"[\\w']+|[.,!?;:-]\"\n",
    "        self.punc_end = ['.','!','?']\n",
    "    \n",
    "    def word_builder(self,s):\n",
    "        d = {}\n",
    "        o = self.order\n",
    "        for each in range(len(s)-o):\n",
    "            pairs = tuple(s[each:each+o])\n",
    "            if pairs in d: d[pairs].append(s[each+o])\n",
    "            else: d[pairs] = [s[each+o]]\n",
    "        return d\n",
    "\n",
    "    #EXAMPLE 1 - splits on space only, no consideration of punctuation\n",
    "    def simple_word_dict(self):\n",
    "        s = self.corpus.split(' ')\n",
    "        d = self.word_builder(s)\n",
    "        return d\n",
    "    \n",
    "    #EXAMPLE 2 - more thought given to punctuation.\n",
    "    def punc_word_dict(self):\n",
    "        s = re.findall(self.punc_split,self.corpus)\n",
    "        d = self.word_builder(s)\n",
    "        return d\n",
    "    '''\n",
    "    - markov_sentence_one and markov_sentence_two are methods to generate random sentences based on corpus\n",
    "        - these methods have multiple optional variables to make the markov model more flexible\n",
    "        - iterations is used for your desired sentence length\n",
    "        - data is the input data you want to use for your project\n",
    "        - starter (sentence_two) is the start word you want to use\n",
    "        - gen is used to build the final sentence after n recursive calls\n",
    "        - extend (sentence_two) gives the user the option to extend sentence until punctuation mark reached\n",
    "    '''\n",
    "    def markov_sentence_one(self,iterations,data=None,begin=None,gen=[]):\n",
    "        '''gen random sentence with no consideration of punctuation, english language or stop words'''\n",
    "        o = self.order\n",
    "        try:\n",
    "            if data is None:\n",
    "                #defaults to punctuation split, which is my standard at least\n",
    "                data = self.punc_word_dict()\n",
    "            if begin is None:\n",
    "                begin = random.choice(list(data.keys()))\n",
    "                gen = [word for word in begin]\n",
    "            begin_step = data[begin]\n",
    "            prec_word = begin[1-o:]\n",
    "            next_word = random.choice(begin_step)\n",
    "            gen.append(next_word)\n",
    "            if o > 1:\n",
    "                next_step = (*prec_word,next_word)\n",
    "            else:\n",
    "                next_step = tuple([next_word])\n",
    "        except KeyError:\n",
    "            return ' '.join(gen)\n",
    "        if iterations > 0:\n",
    "            self.markov_sentence_one(iterations-1,data=data,begin=next_step,gen=gen)\n",
    "        return ' '.join(gen)\n",
    "    \n",
    "    #ITERATION TWO -- A BIT MORE THOUGHT\n",
    "    def markov_sentence_two(self,iterations,starter=None,data=None,\n",
    "                            begin=None,gen=[],cnt=0,extend=False):\n",
    "        '''deals with punctuation a bit more and allows user to extend a sentence'''\n",
    "        o = self.order\n",
    "        if data is None:\n",
    "            #defaults to punctuation split, which is my standard at least\n",
    "            data = self.punc_word_dict()\n",
    "        if not starter is None:\n",
    "            check = starter in it.chain(*list(data.keys()))\n",
    "            if not check: \n",
    "                return \"word not in corpus, please choose again\"\n",
    "            begin = random.choice([k for k in data.keys() if k[-1]==starter])\n",
    "            gen = [starter]\n",
    "        elif begin is None:\n",
    "            #start sentences with a word that's likely a starter\n",
    "            begin = random.choice([k for k in data.keys() if k[0][0].isupper()])\n",
    "            gen = [word for word in begin]\n",
    "        else:\n",
    "            begin = begin\n",
    "        try:\n",
    "            begin_step = data[begin]\n",
    "            prec_word = begin[1-o:]\n",
    "            next_word = random.choice(begin_step)\n",
    "            gen.append(next_word)\n",
    "            if o > 1:\n",
    "                next_step = (*prec_word,next_word)\n",
    "            else:\n",
    "                next_step = tuple([next_word])\n",
    "        except KeyError:\n",
    "            return ' '.join(gen)\n",
    "        if cnt <= iterations:\n",
    "            self.markov_sentence_two(iterations,starter=None,\n",
    "                                     data=data,begin=next_step,\n",
    "                                     gen=gen,cnt=cnt+1,extend=extend)\n",
    "        if extend:\n",
    "            if cnt > iterations and next_word[-1] not in self.punc_end:\n",
    "                self.markov_sentence_two(iterations,starter=None,\n",
    "                                         data=data,begin=next_step,\n",
    "                                         gen=gen,cnt=cnt,extend=extend)\n",
    "        return ' '.join(gen)\n",
    "    \n",
    "    '''\n",
    "    - finally, the similarity method finds the most similar sentence to the autogenerated markov result\n",
    "    - ideally the result of similarity is low, so the data does not overfit\n",
    "    - higher order generally leads to a very similar sentence\n",
    "    - the similarity metric could be vastly improved, but that is out of scope for this analysis\n",
    "    '''\n",
    "    def most_similar_sentence(self,res):\n",
    "        '''\n",
    "        NOTES ON THIS METHOD\n",
    "        - naive TF-IDF test to check for the most similar sentence and evaluate if too similar to markov \n",
    "        - This is a very inefficient way to implement this but works well enough for this project\n",
    "        - The goal of this method is primarily to demonstrate what happens when you increase order too much \n",
    "        '''\n",
    "        analysis = [res] + [i.replace('\\n','') for i in self.sentence_vec] \n",
    "        vec = TVF(min_df=1).fit_transform(analysis)\n",
    "        vals = (vec * vec.T)[0].A[0]\n",
    "        index = np.where(vals==max(vals[1:]))[0][0]\n",
    "        closest_sentence = analysis[index]\n",
    "        return {'markov_sentence': res, 'closest_sentence': closest_sentence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of businesses: 156639\n",
      "Number of reviews: 4736897\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "************** EXAMPLE USAGE BELOW *****************\n",
    "'''\n",
    "\n",
    "'''NOTE -- change your path to wherever you stored the yelp reviews'''\n",
    "os.chdir('{0}{1}'.format(os.path.realpath('..'),'/yelp_dataset'))\n",
    "path = os.getcwd()\n",
    "yelp = YelpData(path)\n",
    "\n",
    "#test on the first 2000 sentences\n",
    "#in theory, we'd want some sort of filtering first. \n",
    "#The filter below could include reviews from hotels to restaurants\n",
    "sentence = yelp.reviews.text.tolist()[:2000]\n",
    "\n",
    "#initiate matrices of different order\n",
    "order_1 = MarkovModel(sentence,1)\n",
    "order_2 = MarkovModel(sentence,2)\n",
    "order_3 = MarkovModel(sentence,3)\n",
    "order_4 = MarkovModel(sentence,4)\n",
    "order_5 = MarkovModel(sentence,5)\n",
    "\n",
    "#some notes\n",
    "print(\"Number of businesses: {}\".format(len(yelp.businesses.index)))\n",
    "print(\"Number of reviews: {}\".format(len(yelp.reviews.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 1 example:\n",
      " {'markov_sentence': 'Spirit of sandwiches felt they tasted exactly as well deserved stopping in the older and decorate .', 'closest_sentence': \"This is definitely a hole in the wall place. I was actually picking up banh mi sandwiches and saw this place. The duck and pork were prepared as well as the places I go to in ATL. I will definitely be stopping by this place again when I'm in the area.\"}\n"
     ]
    }
   ],
   "source": [
    "order_1_s = order_1.markov_sentence_two(5,extend=True)\n",
    "print(\"Order 1 example:\\n {0}\".format(order_1.most_similar_sentence(order_1_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 2 example:\n",
      " {'markov_sentence': 'The Casino Royal . Came here for dinner . The sliders were much happier and better quality food that tasted microwaved .', 'closest_sentence': \"White Castle located inside The Casino Royal. Came to visit from Los Angles and saw there was a White Castle on the strip. Naturally,  we went to get stoned and came to grub.  The burgers tasted just like the frozen ones you buy at the store. Nothing extra,  nothing special. The experience was good and the woman that attended us was cheerful so it was cool over all.... I'd come back again, why not.\"}\n"
     ]
    }
   ],
   "source": [
    "order_2_s = order_2.markov_sentence_two(9,starter='The',extend=True)\n",
    "print(\"Order 2 example:\\n {0}\".format(order_2.most_similar_sentence(order_2_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 3 example:\n",
      " {'markov_sentence': 'A to the design team ! They even have water activities on weds and Fridays weather permitting .', 'closest_sentence': 'My child has attended this school for the last 9 months. Learning ABCs and math and chinese, playing outdoors and indoors.  The School has lots of different age students from when the child is potty trained to kids taking their SATs to adults learning Mandarin.  They even have water activities on weds and Fridays (weather permitting).  I am happy to drop off my kid to school. My kid has learn how to speak basic Mandarin.  There are lots of non asian children in school. Most teachers are bilangual. Lots of mixed asian children too.  The school is very focus on safety.  You need to be buzzed in to get into the building. Each parents sign in and sign out during pick up and drop off.We (including my mom who is a former teacher) checked out three different pre kindergarten for my child and felt this was the best one.  We are happy we choice this school.'}\n"
     ]
    }
   ],
   "source": [
    "order_3_s = order_3.markov_sentence_two(7,starter='A',extend=True)\n",
    "print(\"Order 3 example:\\n {0}\".format(order_3.most_similar_sentence(order_3_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order 4 example:\n",
      " {'markov_sentence': 'We were seated at 11 : 30 . The first thing I noticed here was the decor , which I have never tried before .', 'closest_sentence': 'Terrific service. The place was packed, but we were seated right away. Chips and salsa on the table as we were seated. Great food, big portions, fast service.'}\n"
     ]
    }
   ],
   "source": [
    "order_4_s = order_4.markov_sentence_two(5,extend=True)\n",
    "print(\"Order 4 example:\\n {0}\".format(order_4.most_similar_sentence(order_4_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
